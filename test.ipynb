{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing your install of VGGish\n",
      "\n",
      "Log Mel Spectrogram example:  [[-4.47303259 -4.29463765 -4.14939193 ... -3.97474254 -3.94778045\n",
      "  -3.78685566]\n",
      " [-4.48592983 -4.28831745 -4.13994942 ... -3.98374974 -3.94981089\n",
      "  -3.79512755]\n",
      " [-4.46165595 -4.29335712 -4.14907932 ... -3.96438562 -3.9489109\n",
      "  -3.78621325]\n",
      " ...\n",
      " [-4.46165595 -4.29335712 -4.14907932 ... -3.96438562 -3.9489109\n",
      "  -3.78621325]\n",
      " [-4.46165595 -4.29335712 -4.14907932 ... -3.96438562 -3.9489109\n",
      "  -3.78621325]\n",
      " [-4.46165595 -4.29335712 -4.14907932 ... -3.96438562 -3.9489109\n",
      "  -3.78621325]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/hormone03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py:1694: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n",
      "/var/hormone03/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py:336: UserWarning: `tf.layers.flatten` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Flatten` instead.\n",
      "  warnings.warn('`tf.layers.flatten` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n",
      "VGGish embedding:  [-0.43252084 -0.25330514 -0.03891924 -0.16376027 -0.3499182  -0.59936893\n",
      " -0.05658104  0.16280285 -0.7555176  -0.08260237 -0.03138635 -0.83147156\n",
      " -0.10581692 -0.01420227 -0.1107798  -0.06599119 -0.22666278  0.8060121\n",
      " -0.56459844 -0.07349294 -0.06056742 -0.11864138 -0.26290444 -0.4155161\n",
      " -0.02423218  0.36676204  0.03564948 -0.549977   -0.00279108 -0.28981644\n",
      " -0.57134503  0.3810783   0.1366871   0.9188573   0.80642533 -0.05767322\n",
      " -0.13229543 -0.05044432 -0.22702815  0.04124349  0.7088706  -0.72661525\n",
      "  0.4956671   0.24034092  0.21580261  0.88385975  1.1954073   0.6688216\n",
      "  0.20919633  0.01531461  0.17449082 -0.6544126  -0.15788004  0.25017852\n",
      " -0.26469558 -0.3989996   0.14588487 -0.18502603  0.39927036  0.3041697\n",
      "  0.1294817  -0.11220933 -0.4023689  -0.5374395  -0.36152244 -0.21291585\n",
      "  0.5371816  -0.30606014 -0.08813701  0.04871783  0.42514458  0.18669182\n",
      " -0.17835425 -0.0693139   0.1470107  -0.2758583  -0.25766498  0.69777316\n",
      "  0.45499614  0.05118209 -0.05679127  0.01915687 -0.3735048  -0.19476905\n",
      "  0.5167761   0.5659667   0.65749323 -0.00181185 -0.01737225  0.4086122\n",
      " -0.1984863  -0.69805074 -0.26430076  0.2572072   0.231901    0.23873919\n",
      " -0.12784672 -0.2912716  -0.43531072 -0.1276089  -0.3013382   0.26585317\n",
      " -0.30033335  0.48453143 -0.5391284  -0.3803264   0.19335584 -0.23141013\n",
      "  0.20179856 -0.01324044  0.03102873 -0.61035657 -0.7164675  -0.12141906\n",
      " -0.5206787   0.17772041  0.02538782  0.07066451 -0.01675154 -0.19342308\n",
      " -0.13498671  0.08212645 -0.0704577  -0.10655665 -0.44430846 -0.33278337\n",
      " -0.11444977 -0.2591061 ]\n",
      "Postprocessed VGGish embedding:  [152  67 116 137 136 122 125  76 160 193 111  13 155  83  59   1  44 110\n",
      " 166 162 162 255 255  94  47 192 119 163 182  27  23  33 153  42 153 255\n",
      "  10  57 255   2 133 164 232 186 255 134  84  75 107 255   0 255 113   0\n",
      " 169 212  85 255  43   0 255   0   1 255 180   0  46 255  26  86  49 161\n",
      "  86 255 129 121 255 167 212 174  19   0 255 175  57 255   0 255   0 103\n",
      "   0  11 220  40 255 245   0 255  49   0 255  53 215 255  84   0  49 214\n",
      " 209   0 255   0   0   9 207   0 255   0 160 232   0 255   0 129   0 255\n",
      "   0 192]\n",
      "\n",
      "Looks Good To Me!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.feature\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "\n",
    "#from scripts\n",
    "from Attention import *\n",
    "from arguments import parse_arguments \n",
    "import torch\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from Attention import *\n",
    "from data_utils import *\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "\n",
    "from vggish_smoke_test import *;\n",
    "import vggish_slim\n",
    "import vggish_params\n",
    "import vggish_input\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateVGGishNetwork(hop_size=0.96):   # Hop size is in seconds.\n",
    "  \"\"\"Define VGGish model, load the checkpoint, and return a dictionary that points\n",
    "  to the different tensors defined by the model.\n",
    "  \"\"\"\n",
    "  vggish_slim.define_vggish_slim()\n",
    "  checkpoint_path = 'vggish_model.ckpt'\n",
    "  vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
    "  vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "  features_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.INPUT_TENSOR_NAME)\n",
    "  embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "      vggish_params.OUTPUT_TENSOR_NAME)\n",
    "  layers = {'conv1': 'vggish/conv1/Relu',\n",
    "            'pool1': 'vggish/pool1/MaxPool',\n",
    "            'conv2': 'vggish/conv2/Relu',\n",
    "            'pool2': 'vggish/pool2/MaxPool',\n",
    "            'conv3': 'vggish/conv3/conv3_2/Relu',\n",
    "            'pool3': 'vggish/pool3/MaxPool',\n",
    "            'conv4': 'vggish/conv4/conv4_2/Relu',\n",
    "            'pool4': 'vggish/pool4/MaxPool',\n",
    "            'fc1': 'vggish/fc1/fc1_2/Relu',\n",
    "            #'fc2': 'vggish/fc2/Relu',\n",
    "            'embedding': 'vggish/embedding',\n",
    "            'features': 'vggish/input_features',\n",
    "         }\n",
    "  g = tf.get_default_graph()\n",
    "  for k in layers:\n",
    "    layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
    "  return {'features': features_tensor,\n",
    "          'embedding': embedding_tensor,\n",
    "          'layers': layers,\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessWithVGGish(vgg, x, sr):\n",
    "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
    "  (sr). Return a whitened version of the embeddings. Sound must be scaled to be\n",
    "  floats between -1 and +1.'''\n",
    "  # Produce a batch of log mel spectrogram examples.\n",
    "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "  [embedding_batch] = sess.run([vgg['embedding']],\n",
    "                               feed_dict={vgg['features']: input_batch})\n",
    "  # Postprocess the results to produce whitened quantized embeddings.\n",
    "  pca_params_path = 'vggish_pca_params.npz'\n",
    "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "  # print('Postprocessed VGGish embedding: ', postprocessed_batch[0])\n",
    "  return postprocessed_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ProcessWithVGGish(vgg, x, sr):\n",
    "  '''Run the VGGish model, starting with a sound (x) at sample rate\n",
    "  (sr). Return a whitened version of the embeddings. Sound must be scaled to be\n",
    "  floats between -1 and +1.'''\n",
    "  # Produce a batch of log mel spectrogram examples.\n",
    "  input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "  # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "  [embedding_batch] = sess.run([vgg['embedding']],\n",
    "                               feed_dict={vgg['features']: input_batch})\n",
    "  # Postprocess the results to produce whitened quantized embeddings.\n",
    "  pca_params_path = 'vggish_pca_params.npz'\n",
    "  pproc = vggish_postprocess.Postprocessor(pca_params_path)\n",
    "  postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "  # print('Postprocessed VGGish embedding: ', postprocessed_batch[0])\n",
    "  return postprocessed_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "# Test these new functions with the original test.\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "vgg = CreateVGGishNetwork(0.21);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "#Video(\"multiple3.mp4\", embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v, sr_v  = librosa.load('/private/var/hormone03/Dropbox/My Mac (Macâ€™s MacBook Pro)/Desktop/avid/instrumentClassification/AttentionMIC_master/multiple3.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_v = _ProcessWithVGGish(vgg, y_v, sr_v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1397, 128)\n"
     ]
    }
   ],
   "source": [
    "#X_v.shape\n",
    "k_v1, k_v2 = X_v.shape\n",
    "X_v = np.asarray(X_v).reshape(-1,k_v1,k_v2)\n",
    "print(X_v.shape)\n",
    "# Your code here. Aim for 2-4 lines.\n",
    "X_tst_torch_v = (torch.from_numpy(X_v)).type(torch.float32)\n",
    "#y_tst_torch_v = torch.from_numpy(y_tst).type(torch.float32)\n",
    "X_tst_torch_v =(X_tst_torch_v/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55336\n",
      "(20,)\n",
      "[8.4661275e-02 4.5212246e-03 6.8876579e-02 7.0413955e-02 1.0355338e-01\n",
      " 7.2207522e-01 4.1174740e-01 5.2094662e-01 2.6439232e-01 3.4639583e-05\n",
      " 1.6299731e-03 9.8015286e-04 3.3898365e-01 9.9564481e-01 1.5991399e-01\n",
      " 9.9848616e-01 9.9915504e-01 1.6879686e-03 4.7329325e-02 9.2277890e-01]\n",
      "(array([13, 15, 16, 19]),)\n"
     ]
    }
   ],
   "source": [
    "modelPath = '/private/var/hormone03/Dropbox/My Mac (Macâ€™s MacBook Pro)/Desktop/avid/instrumentClassification/AttentionMIC_master/trainedModel_0.pth'\n",
    "\n",
    "model = DecisionLevelSingleAttention(\n",
    "                freq_bins=128,\n",
    "                classes_num=20,\n",
    "                emb_layers=3,\n",
    "                hidden_units=128,\n",
    "                drop_rate=0.6)\n",
    "\n",
    "\n",
    "#Restore model\n",
    "model.load_state_dict(torch.load(modelPath))\n",
    "model = model.eval()\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_v = model(X_tst_torch_v)\n",
    "\n",
    "output_v = to_numpy(output_v).ravel()\n",
    "print(output_v.shape)\n",
    "print(output_v) \n",
    "outputIndex = np.where(output_v >= 0.9)\n",
    "print(outputIndex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classMap= {'accordion': 0,\n",
    " 'banjo': 1,\n",
    " 'bass': 2,\n",
    " 'cello': 3,\n",
    " 'clarinet': 4,\n",
    " 'cymbals': 5,\n",
    " 'drums': 6,\n",
    " 'flute': 7,\n",
    " 'guitar': 8,\n",
    " 'mallet_percussion': 9,\n",
    " 'mandolin': 10,\n",
    " 'organ': 11,\n",
    " 'piano': 12,\n",
    " 'saxophone': 13,\n",
    " 'synthesizer': 14,\n",
    " 'trombone': 15,\n",
    " 'trumpet': 16,\n",
    " 'ukulele': 17,\n",
    " 'violin': 18,\n",
    " 'voice': 19}\n",
    "inv_map = {v: k for k, v in classMap.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "saxophone\n",
      "15\n",
      "trombone\n",
      "16\n",
      "trumpet\n",
      "19\n",
      "voice\n"
     ]
    }
   ],
   "source": [
    "for i in outputIndex[0]:\n",
    "    print(i)\n",
    "    print(inv_map[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-e6e4daba5a09>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-e6e4daba5a09>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    accordion 0.06983575\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "accordion 0.06983575\n",
    "banjo 0.33101374\n",
    "bass 0.020017007\n",
    "cello 0.06161041\n",
    "clarinet 0.12218426\n",
    "cymbals 0.0016985144\n",
    "drums 0.014519502\n",
    "flute 0.700888\n",
    "guitar 0.8985557\n",
    "mallet_percussion 0.5362431\n",
    "mandolin 0.59339917\n",
    "organ 0.045906793\n",
    "piano 0.7231139\n",
    "saxophone 0.0318367\n",
    "synthesizer 0.23019123\n",
    "trombone 0.010454232\n",
    "trumpet 0.025121506\n",
    "ukulele 0.4342751\n",
    "violin 0.25603348\n",
    "voice 0.0148326345\n",
    "\n",
    "flute, guitar,mallet_percussion, piano, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}